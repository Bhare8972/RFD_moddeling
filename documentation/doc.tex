\documentclass[]{article}
\usepackage{ txfonts  }


%opening
\title{Documentation on simulating RREA and RFD}
\author{Brian Hare}

\begin{document}

\maketitle

%\begin{abstract}

%\end{abstract}

\section{table of constants}

\begin{center}
	\begin{tabular}{ c c c }
		Name                          & symbol    &   value  \\ 
		speed of light                & C         &         \\  
		charge of electron            & -e        &         \\  
		molecular density of air      & $N_m$     &   $2.688\times 10^{25} m^{-3} $     \\  
		average nuclear charge of air & $Z_m$     &  14.5       \\  
		classical electron radius     & $r_e$     &   $2.8179\times 10^{-15} m $      \\  
        mass of electron              & $m_e$     &     \\
        ionization potential of air   & $I$       &  85.7 ev    \\  
	\end{tabular}
\end{center}


\section{dimensionless variables}

Dimensionless variables are used in this simulation. This can complicate taking a physical equation, and relating it to a formula that can be used in the simulation, but once in the simulation the formulas tend to be simpler, thus easier to use. In the rest of this documentation, the normal symbols, e.g. $\varepsilon$ for kinetic energy and $ \vec{P} $ for momentum, will represent the values of those quantities in MKS units, alternate symbols will be used to represent the values of each quantity in dimensionless units, e.g. $E$ for kinetic energy and $\vec{\rho} $ for momentum. The table below gives the units of all the dimensionless variables used in the simulation. Brackets around a symbol, e.g.  $\left[ E\right] $ for energy and $\left[  \vec{\rho} \right] $ for momentum, represents the units of the dimensionless values, e.g. $\varepsilon = E \left[ E \right] $.

\begin{center}
	\begin{tabular}{ c c c }
		Name                 & units                             &   alternate symbol  \\ 
		time                 & $(2\pi N_m Z_m r^2_e C)^{-1}$     & $ \tau$    \\  
		velocity             & C                                 &  $\vec{\beta}$       \\  
		position             & $C\times \left[ \tau \right] $    & $ \vec{\chi} $     \\
		momentum             & $m_e C $                          &  $\vec{\rho} $     \\
		energy               & $m_e C^2 $                        &  E (kinetic only)      \\
		force                & $\frac{m_e C}{ \left[ \tau \right]} $     & $ \frac{d\vec{\rho}}{d\tau}$       \\
		electric field       & $\frac{m_e C}{ e \left[ \tau \right]} $   & $  \vec{\xi}  $   \\
		magnetic field       & $\frac{m_e }{ e \left[ \tau \right]} $    &  $ \vec{\Upsilon} $     \\
	\end{tabular}
\end{center}

\section{relativistic equations}

This simulation deals with very relativistic particles, and so must use relativistically correct formula. Since, in the simulation, each particle stores position and momentum, of particular interest is simple formula relating momentum to other quantities.

First, is the definition of gamma:
\begin{equation}
\gamma = \frac{1}{ \sqrt{1-\beta^2 } }
\end{equation}

Allowing us to define total energy:
\begin{equation}
\varepsilon_t = \gamma m_e C^2
\end{equation}

or in dimensionless units:

\begin{equation}
E_t = \gamma 
\end{equation}

The definition of kinetic energy:
\begin{equation}
\varepsilon_t = m_e C^2 + \varepsilon
\end{equation}

and in dimensionless units:
\begin{equation}
E_t = 1 + E
\end{equation}

Substitution gives us:
\begin{equation}
E=\gamma-1
\end{equation}

Next is the definition of momentum:

\begin{equation}
\vec{P} = \gamma m_e \vec{V}
\end{equation}

In dimensionless units:

\begin{equation}
\vec{\rho}= \gamma \vec{\beta}
\end{equation}

We can relate momentum to energy:
\begin{equation}
\varepsilon_t^2=(M_eC^2)^2 + (PC)^2
\end{equation}

dimensionless:
\begin{equation}
E_t^2=1+\rho^2
\end{equation}

Or:
\begin{equation}
\gamma^2=1+\rho^2
\end{equation}

thus:
\begin{equation}
E=\sqrt{1+\rho^2}-1
\end{equation}

Substitution gives:
\begin{equation}
\beta^2=\frac{\rho^2}{1+\rho^2}
\end{equation}


\section{forces and equations of motion}

The simplest task that needs to be done in the simulation, is to simulate the motion of a particle given a electric field, magnetic field, and friction.

In dimensionless units, the equations of motion for an electron are:

\begin{equation}
\frac{d \vec{\rho}}{ d \tau} = -\vec{\xi}(\vec{\chi}) -\frac{\vec{\rho} \times \vec{\Upsilon}(\vec{\chi}) }{\sqrt{ 1 + \rho^2 }} + \hat{\rho} \frac{d \rho}{d \tau}_{friction}(\rho)
\end{equation}

and 

\begin{equation}
\frac{d \vec{\chi}}{d \tau} = \frac{\vec{\rho}}{\sqrt{1+\rho^2}}
\end{equation}

This are simulated using a fourth-order runge-kutta technique. By first combining the two equations of motion into one:


\begin{equation}
\frac{d}{d \tau}  
\left[ \begin{array}{c}
\vec{ \chi }\\
\vec{\rho} \\
\end{array} \right]
=
\left[ \begin{array}{c}
g(\tau, S )\\
f(\tau, S ) \\
\end{array} \right]
\end{equation}

We define the vector S so that:

\begin{equation} 
\frac{d}{d \tau} S = S'
\end{equation}

Then we can find S at the next time step, given a stepping time of $\Delta \tau$:

\begin{equation}
S_{n+1} = S_n + \frac{\Delta\tau}{6} ( K_1 + 2K_2 + 2K_3 + K_4 )
\end{equation}

where:

\begin{equation} 
K_1=S'(\tau_n, S_n)
\end{equation}

\begin{equation} 
K_2=S'(\tau_n + \frac{\Delta\tau}{2}, S_n + \frac{\Delta\tau}{ 2}K_1)
\end{equation}

\begin{equation} 
K_3=S'(\tau_n + \frac{\Delta\tau}{2}, S_n + \frac{\Delta\tau}{ 2}K_2)
\end{equation}

\begin{equation} 
K_4=S'(\tau_n + \Delta\tau, S_n + \Delta\tau K_3)
\end{equation}

Given that high-precision is not really needed in solving the equations of motion, 4th order runge-kutta may be overkill.  Work will need to be done to see if the time required to use 4th order runga-kutta is significant compared to the time required to calculate scattering processes. If so, maybe I should consider using a lower-order scheme.

The frictional force due to ionization could be approximated using the Bethe formula, this however, produces some problems as the Bethe formula is at best an approximation, which is problematic because we care about finding the minimum electric field necessary for RREA and for RFD, which is sensitive to the energy loss due to ionization, and more worrisome is that the Bethe formula is very imprecise and unstable at low energies, and there are a large number of low energy electrons in RREA. The solution is to use tabulated values instead. Particularly, we use tabulated values from ICRU report 37, which gives the energy loss due to ionization for electrons and positrons down to a very low energy. These tables are hard-coded into a program that is separate from the main simulation. When run, this program converts the values into dimensionless units, and it converts the energy values into dimensionless momentum squared (because momentum squared is easier to calculate at runtime than energy), and stores the two tables into a binary file. At startup, the simulation loads the energy losses due to ionization tables from the binary file, then during runtime these tables can be searched and energy loss rates returned using linear interpolation. At the time of writing this, the time required to search the lookup table is, at absolute worse, twice as slow as calculating the values using the Bethe formula. It is doubtful that the algorithm can be improved, but I believe that the increase in stability at low energy warrants the slowdown, and that the slowdown will be minor compared to the time to calculate other scattering  processes (this has yet to be shown). 

The tables in ICRU 37 extend to very high energy, and so probably will not need to generate new values using the Beth formula. However, as we include Moller scattering in the simulation, the ionization table generation program will need to take Moller scattering into account by subtracting off the energy loss due to Moller scattering at the appropriate energies. This has not been implemented.

%The frictional force is given by the Bethe equation:
%
%\begin{equation} 
%F_{friction}(P)  =\frac{ 2 \pi N_m Z_m r_e^2 m C^2}{\beta^2} \left\lbrace  \ln\frac{mv^2\varepsilon\gamma}{I^2}
%-\left( 1 + \frac{2}{\gamma}  - \frac{1}{\gamma^2} \right)\ln 2 + \frac{(\gamma-1)^2}{8\gamma^2}  + \frac{1}{\gamma^2} \right\rbrace 
%\end{equation}
%
%in dimensionless units:
%
%\begin{equation} 
%\frac{d \rho}{d \tau}_{friction}(\rho)  =\frac{ 1 }{\beta^2} \left\lbrace  \ln\frac{\beta^2E\gamma}{\bar{I^2}}
%-\left( 1 + \frac{2}{\gamma}  - \frac{1}{\gamma^2} \right)\ln 2 + \frac{(\gamma-1)^2}{8\gamma^2}  + \frac{1}{\gamma^2} \right\rbrace 
%\end{equation}
%
%Where $\bar{I}$ is $I/m_eC^2$.
%
%Since Moller scatter is considered for electrons that are above a  threshold kinetic energy of $E_{thresh}= \frac{2 keV}{m_e C^2}$, the energy loss due to moller scattering must be subracted off from the friction force. So, for electrons with energies above $2E_{thresh}$, the following friction function is used instead of the one above:
%
%In dimensionless units:
%
%\begin{equation} 
%\frac{d \rho}{d \tau}_{friction}(\rho)  =\frac{ 1 }{\beta^2} \left\lbrace  \ln\frac{2E_{thresh} \beta^2 \gamma}{\bar{I^2}}
%-\left( 1 + \frac{2}{\gamma}  - \frac{1}{\gamma^2} \right)\ln \frac{ E }{E- E_{thresh}} + \frac{E_{thresh}}{E-E_{thresh}} - \beta^2 + \frac{E_{thresh}^2}{2\gamma^2} \right\rbrace 
%\end{equation}


\section{shielded coulomb scattering}

As the electrons and positrons travel through the simulation, they will collide off of atomic nuclei. We simulate this by including the effects of elastic scattering off of atomic nuclei via the shielded coulomb cross section. The differential cross section for the shielded coulomb cross section is:

\begin{equation} 
\frac{d\sigma_{Coul}}{d \Omega} = \frac{1}{4}\left(  \frac{Z_m r_e }{\beta^2 \gamma }  \right)^2\frac{1-\beta^2\sin^2(\theta/2)}{ \left(  \sin^2(\theta/2) + \frac{\hbar}{4P^2a^2}  \right)^2 }
\end{equation}

where
\begin{equation} 
a=183.8 \lambdabar z_m^{-1/3}
\end{equation}

Using the definition of cross-sections, we can relate the differential cross section to expected number of times a particle will be deflected into an angle:

\begin{equation} 
\frac{dn}{d \Omega} =N_m Z_m V \Delta T \frac{d \sigma}{d \Omega}
\end{equation}

where $\frac{dn}{d \Omega}$ is the expected number of times that a particle will be deflected by angle $\Omega$ during time $\Delta T$. If $\frac{dn}{d \Omega} << 1$, it can be interpreted as a probability. 

In dimensionless units, this turns into:

\begin{equation} 
\frac{dn}{d \Omega} =\frac{ \beta \Delta \tau }{2 \pi r_e^2} \frac{d \sigma}{d \Omega}
\end{equation}

Plugging in the differential cross section for elastic shielded coulomb scattering gives:

\begin{equation}
\label{coulomb_scattering_rate_eq}
\frac{dn}{d \Omega} =\frac{ \Delta \tau }{8 \pi \beta }\left( \frac{Z_m}{\rho} \right)^2  \frac{1-\beta^2\sin^2(\theta/2)}{ \left(  \sin^2(\theta/2) + \frac{Z_m^{2/3}}{4\times 183.8^2} \frac{1}{\rho^2} \right)^2 }
\end{equation}

Two decisions need to be made using this formula. 1) How many times does a particle scatter in one time step, and 2) what angles does it scatter into? The first question can be answered by integrating equation \ref{coulomb_scattering_rate_eq} over the unit sphere for a given particle energy and time step, then by sampling the Poisson distribution. For shielded coulomb scattering, this turns out to be many many times for any reasonable time step. The second question is answered using inverse transform sampling. 

Inverse transform sampling is performed by first integrating the distribution over the relevant variable that needs to be sampled:

\begin{equation}
N(\theta) = \int_0^{\theta} sin(\theta ) d\theta '  \int_0^{2\pi} d\phi ' \frac{dn}{d\Omega}
\end{equation}

Normalizing and inverting the function such that:

\begin{equation}
N'\left(  \frac{N(\theta)}{N(\pi)}  \right) = \theta
\end{equation}

Then we can then sample a number between 0 to 1, from the uniform distribution, $U$, and plug it into $N'$ to get a sample for our variable:

\begin{equation}
N'\left(  U  \right) = \theta
\end{equation}

Except for a few specific cases, inverse transform sampling cannot be done analytically. I will describe a method I have developed to do inverse transform sampling reliably. After sampling the inclination, the azimuth, $\phi$ can be sampled from the uniform distribution. 

After the inclination and azimuth have been sampled, we can rotate the momentum vector by finding an orthogonal, but not normal, vector basis in which the momentum is one axis. First, we use cross products to find two vectors, $\vec{Bv}$ and $\vec{Cv}$ orthogonal to each other, and orthogonal to the momentum vector, $\vec{\rho}$:

\begin{equation}
\label{Bv_def}
\vec{Bv}=\frac{\vec{\varsigma}\times \vec{\rho}}{ \left| \vec{\varsigma}\times \vec{\rho}  \right|   }
\end{equation}

Where $\vec{\varsigma}$ is any unit vector chosen so that equation \ref{Bv_def} is not singular.

\begin{equation}
\vec{Cv}=\vec{Bv}\times \vec{\rho}
\end{equation}

$\vec{Cv}$ is guaranteed to have the same magnitude as the momentum, since $\vec{Bv}$ is guaranteed to be a unit vector orthogonal to the momentum.

Using these new basis vectors, we can calculate the new momentum:

\begin{equation}
\vec{\rho}_{new}= cos(\theta) \vec{\rho} + sin(\theta) cos(\phi) \vec{Bv} \left|\vec{\rho} \right| - sin(\theta) sin(\phi) \vec{Cv} 
\end{equation}

This process needs to be ran however many times scattering happened to occur for that time step. This process is very slow. So what is done, is that this process is ran many many times before the simulation, and a distribution for $\theta$ is built up for different energies and time steps. This distribution is then saved in a table and can be opened and sampled relatively quickly at simulation time.


\section{numerical inverse transform sampling}

My method for inverse transform sampling relies upon quadratic interpolation. We need a quadratic interpolant:

\begin{equation}
\label{second_order_interpolant_eq}
\bar{Y}(x)=W_1 +W_2 x + W_3 x^2
\end{equation}

Such that $\bar{Y}$ is precise at three points:

\[ \bar{Y}(X_L)=Y_L \]
\[ \bar{Y}(X_M)=Y_M \]
\[ \bar{Y}(X_R)=Y_R \]

Solving the 3x3 matrix reveals:

\begin{equation}
W_3=\frac{(X_M-X_L)(Y_R-Y_L) - (X_R-X_L)(Y_M-Y_L)  }{ (X_M-X_L)(X_R^2-X_L^2) - (X_R-X_L)(X_M^2-X_L^2)   }
\end{equation}

\begin{equation}
W_2=\frac{  Y_M - Y_L  }{ X_M-X_L  } - W_3 \frac{X_M^2-X_L^2}{X_M-X_L}
\end{equation}

\begin{equation}
W_1=Y_L - W_2X_L - W_3X_L^2
\end{equation}

Integrating this interpolant gives us an approximation for the cumulative integral of our function;

\begin{equation}
\label{quadrature_eq}
\int_{X_L}^{x} Y(x') dx' \approx \int_{X_L}^{x} \bar{Y}(x') dx' = W_1x + \frac{W_2}{2}x^2 + \frac{W_3}{3}x^3 -(W_1X_L + \frac{W_2}{2}X_L^2 + \frac{W_3}{3}X_L^3)
\end{equation}

Using this integral over multiple regions, we can have a method adaptive cumulative quadrature. This is done by first splitting the region we want to integrate over into multiple sections (say, 5). We then sample the function we want to integrate at the endpoints and at the middle of each section and estimate the area of each section using equation \ref{quadrature_eq}. We then split each section into a left and right subsections, and sample the function at the center of each subsection to get a slightly better estimate of the area.  Finally we test our precision using equation \ref{quadrature_condition_eq} below. Where $float(x)$ is the floating point representation of x. $A_s$, $A_l$, and $A_r$ are the areas of some section and its left and right subsections, and $F$ is a factor that controls our precision. 

\begin{equation}
\label{quadrature_condition_eq}
float(F*A_s + (A_s - A_l - A_r)) == float(F*A_s)
\end{equation}

For any subsection, if this condition is true, then we are done and move onto the next section. If this condition is not true then the two subsections are each split into two subsections and this procedure is repeated until condition \ref{quadrature_condition_eq} is true. In this way we can very precisely sample the integral of any function at a number of un-equally spaced points. I am not sure how this method will behave if the value of the integral is at or close to zero. I am using low-order polynomial quadrature instead of a more advance method, say a high-order Gaussian quadrature, because this method is relatively simple and easily produced a cumulative integral.

Finally, this cumulative integral is normalized and is inverted by plugging the value of the integral at each point into our interpolant (equation \ref{second_order_interpolant_eq}) for the x values and the points that the integral was sampled at for the y-values. We can then sample our distribution by sampling a variable from the uniform distribution and plugging that number into this inverse interpolant. This method can only sample one-dimensional distributions, but multi-dimensional distributions can always be turned into multiple one-dimensional distributions via integration.
\end{document}
